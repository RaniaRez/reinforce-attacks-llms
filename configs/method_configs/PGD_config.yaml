default_method_hyperparameters:  # For full config see implementation
  num_test_cases_per_behavior: 1
  test_cases_batch_size: 34
  num_steps: 5_000
  allow_non_ascii: False
  use_prefix_cache: True
  targets_path: ./data/optimizer_targets/harmbench_targets_text.json
  eval_steps: 250

# ========== Specific experiment hyperparameters (overrides default_method_hyperparameters) ========== #
# Experiment names can be used in the save path and to override default hyperparameters.
# To add a new experiment, add a new section with the following format:
# 
# experiment_name:
#   <hyperparameter>: <value>
#   ...
#   <hyperparameter>: <value>
# 
# Experiment name parsing:
# If an experiment name contains <model_name#>, then it will be expanded out into all model names in the models.yaml file.
# The parameters in these experiment configs can have the form <model_name#>['<key1>'][index1]['<key2>']... to reference values in the corresponding model config.
# If an expanded experiment name would have the same name as a manually added experiment, the manually added experiment config will be used instead.

<model_name1>:
  target_model: <model_name1>['model']

orca_2_7b_custom_targets:  # Uses targets tailored for Orca
  target_model:
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: False
    dtype: bfloat16
    chat_template: <model_name1>['model']['chat_template']
  targets_path: ./data/optimizer_targets/extra_targets/harmbench_targets_text_orca.json

orca_2_7b_custom_targets:  # Uses targets tailored for Orca
  test_cases_batch_size: 34
  target_model:
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: False
    dtype: bfloat16
    chat_template: <model_name1>['model']['chat_template']
  targets_path: ./data/optimizer_targets/extra_targets/harmbench_targets_text_orca.json

orca_2_13b_custom_targets:  # Uses targets tailored for Orca
  test_cases_batch_size: 17
  target_model:
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: False
    dtype: bfloat16
    chat_template: <model_name1>['model']['chat_template']
  targets_path: ./data/optimizer_targets/extra_targets/harmbench_targets_text_orca.json

baichuan2_7b:
  test_cases_batch_size: 34
  target_model: <model_name1>['model']
  use_prefix_cache: False  # Baichuan 2 expects KV cache to only be used during generation  
  check_refusal_min_loss: 0.5

baichuan2_13b:
  test_cases_batch_size: 17
  target_model: <model_name1>['model']
  use_prefix_cache: False  # Baichuan 2 expects KV cache to only be used during generation  
  check_refusal_min_loss: 0.5

koala_7b:
  test_cases_batch_size: 34
  target_model: <model_name1>['model']

koala_13b:
  test_cases_batch_size: 17
  target_model: <model_name1>['model']

qwen_7b_chat:
  test_cases_batch_size: 12
  target_model: <model_name1>['model']
  use_prefix_cache: False  # Unknown error causes issues when using KV cache during optimization with Qwen

qwen_14b_chat:
  test_cases_batch_size: 17
  target_model: <model_name1>['model']
  use_prefix_cache: False  # Unknown error causes issues when using KV cache during optimization with Qwen

qwen_72b_chat:
  test_cases_batch_size: 12
  target_model: <model_name1>['model']
  use_prefix_cache: False  # Unknown error causes issues when using KV cache during optimization with Qwen

llama2_7b:
  test_cases_batch_size: 34
  target_model:
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: <model_name1>['model']['use_fast_tokenizer']
    dtype: bfloat16
    chat_template: <model_name1>['model']['chat_template']

llama2_13b:
  test_cases_batch_size: 17
  target_model:
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: <model_name1>['model']['use_fast_tokenizer']
    dtype: bfloat16
    chat_template: <model_name1>['model']['chat_template']

llama2_70b:
  test_cases_batch_size: 12
  target_model:
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: <model_name1>['model']['use_fast_tokenizer']
    dtype: bfloat16
    chat_template: <model_name1>['model']['chat_template']

mixtral_8x7b:
  test_cases_batch_size: 34
  target_model: <model_name1>['model']

# Vicuna
vicuna_7b_v1_5:
  test_cases_batch_size: 34
  target_model:
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: <model_name1>['model']['use_fast_tokenizer']
    dtype: bfloat16
    chat_template: <model_name1>['model']['chat_template']

vicuna_13b_v1_5:
  test_cases_batch_size: 17
  target_model:
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: <model_name1>['model']['use_fast_tokenizer']
    dtype: bfloat16
    chat_template: <model_name1>['model']['chat_template']

# SOLAR
solar_10_7b_instruct:
  test_cases_batch_size: 34
  target_model:
    model_name_or_path: <model_name1>['model']['model_name_or_path']
    use_fast_tokenizer: <model_name1>['model']['use_fast_tokenizer']
    dtype: bfloat16
    chat_template: <model_name1>['model']['chat_template']
